{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, optimizers, regularizers\n",
    "from tensorflow.keras.models import Sequential\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pathlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_url = \"https://storage.googleapis.com/download.tensorflow.org/example_images/flower_photos.tgz\"\n",
    "data_dir = tf.keras.utils.get_file('flower_photos', origin=dataset_url, untar=True)\n",
    "data_dir = pathlib.Path(data_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train set size: 2569\n",
      "Validation set size: 550\n",
      "Test set size: 551\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "data_dir = 'C:/Users/hecr/.keras/datasets/flower_photos'\n",
    "\n",
    "# Load all file paths and labels\n",
    "image_paths = []\n",
    "labels = []\n",
    "class_names = sorted([d for d in os.listdir(data_dir) if os.path.isdir(os.path.join(data_dir, d))])\n",
    "\n",
    "for label, class_name in enumerate(class_names):\n",
    "    class_dir = os.path.join(data_dir, class_name)\n",
    "    for img_name in os.listdir(class_dir):\n",
    "        img_path = os.path.join(class_dir, img_name)\n",
    "        if os.path.isfile(img_path):\n",
    "            image_paths.append(img_path)\n",
    "            labels.append(label)\n",
    "\n",
    "# Convert lists to numpy arrays for splitting\n",
    "image_paths = np.array(image_paths)\n",
    "labels = np.array(labels)\n",
    "\n",
    "# Split data into training (80%), validation (10%), and test (10%) sets\n",
    "train_paths, temp_paths, train_labels, temp_labels = train_test_split(image_paths, labels, test_size=0.3, stratify=labels, random_state=42)\n",
    "val_paths, test_paths, val_labels, test_labels = train_test_split(temp_paths, temp_labels, test_size=0.5, stratify=temp_labels, random_state=42)\n",
    "\n",
    "# Helper function to create TensorFlow datasets\n",
    "def create_dataset(image_paths, labels, batch_size=16):\n",
    "    dataset = tf.data.Dataset.from_tensor_slices((image_paths, labels))\n",
    "\n",
    "    def load_image(path, label):\n",
    "        img = tf.io.read_file(path)\n",
    "        img = tf.image.decode_jpeg(img, channels=3)\n",
    "        img = tf.image.resize(img, (224, 224))\n",
    "        return img, label\n",
    "\n",
    "    dataset = dataset.map(load_image, num_parallel_calls=tf.data.experimental.AUTOTUNE)\n",
    "    dataset = dataset.shuffle(1000).batch(batch_size).prefetch(tf.data.experimental.AUTOTUNE)\n",
    "    return dataset\n",
    "\n",
    "# Create datasets\n",
    "train_dataset = create_dataset(train_paths, train_labels)\n",
    "val_dataset = create_dataset(val_paths, val_labels)\n",
    "test_dataset = create_dataset(test_paths, test_labels)\n",
    "\n",
    "# Optionally print the sizes\n",
    "print(f'Train set size: {len(train_paths)}')\n",
    "print(f'Validation set size: {len(val_paths)}')\n",
    "print(f'Test set size: {len(test_paths)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_augmentation = tf.keras.Sequential(\n",
    "  [\n",
    "    tf.keras.layers.Rescaling(1./255),\n",
    "    layers.RandomFlip(\"horizontal\",\n",
    "                      input_shape=(224,\n",
    "                                  224,\n",
    "                                  3)),\n",
    "    layers.RandomRotation(0.1),\n",
    "  ]\n",
    ")\n",
    "model = Sequential([\n",
    "  data_augmentation,\n",
    "  layers.Conv2D(32, (3, 3), activation='relu'),\n",
    "  layers.MaxPooling2D((2, 2)),\n",
    "  layers.Conv2D(32, (3, 3), activation='relu'),\n",
    "  layers.MaxPooling2D((2, 2)),\n",
    "  layers.Conv2D(32, (3, 3), activation='relu'),\n",
    "  layers.MaxPooling2D((2, 2)),\n",
    "  layers.Flatten(),\n",
    "  layers.Dense(64, activation='relu', activity_regularizer=regularizers.L2(0.01)),\n",
    "  layers.Dropout(0.1),\n",
    "  layers.Dense(5, name=\"outputs\", activation=\"sigmoid\")\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizerr = optimizers.Adam(learning_rate=0.005)\n",
    "\n",
    "# Compile the model with the optimizer object\n",
    "model.compile(optimizer=\"adam\", loss='sparse_categorical_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "184/184 [==============================] - 105s 562ms/step - loss: 1.3208 - accuracy: 0.4768 - val_loss: 1.2488 - val_accuracy: 0.5504\n",
      "Epoch 2/20\n",
      "184/184 [==============================] - 106s 574ms/step - loss: 1.1151 - accuracy: 0.5916 - val_loss: 1.1675 - val_accuracy: 0.5531\n",
      "Epoch 3/20\n",
      "184/184 [==============================] - 106s 575ms/step - loss: 1.0103 - accuracy: 0.6434 - val_loss: 1.0315 - val_accuracy: 0.6349\n",
      "Epoch 4/20\n",
      "184/184 [==============================] - 108s 583ms/step - loss: 0.9323 - accuracy: 0.6744 - val_loss: 0.9540 - val_accuracy: 0.6649\n",
      "Epoch 5/20\n",
      "184/184 [==============================] - 103s 557ms/step - loss: 0.8774 - accuracy: 0.7027 - val_loss: 0.9020 - val_accuracy: 0.7084\n",
      "Epoch 6/20\n",
      "184/184 [==============================] - 103s 558ms/step - loss: 0.8398 - accuracy: 0.7119 - val_loss: 0.8731 - val_accuracy: 0.7193\n",
      "Epoch 7/20\n",
      "184/184 [==============================] - 102s 555ms/step - loss: 0.8006 - accuracy: 0.7316 - val_loss: 0.8573 - val_accuracy: 0.7057\n",
      "Epoch 8/20\n",
      "184/184 [==============================] - 106s 572ms/step - loss: 0.7791 - accuracy: 0.7476 - val_loss: 0.8839 - val_accuracy: 0.6948\n",
      "Epoch 9/20\n",
      "184/184 [==============================] - 105s 570ms/step - loss: 0.7453 - accuracy: 0.7480 - val_loss: 0.7717 - val_accuracy: 0.7466\n",
      "Epoch 10/20\n",
      "184/184 [==============================] - 103s 559ms/step - loss: 0.7064 - accuracy: 0.7667 - val_loss: 0.8495 - val_accuracy: 0.7139\n",
      "Epoch 11/20\n",
      "184/184 [==============================] - 102s 553ms/step - loss: 0.6853 - accuracy: 0.7854 - val_loss: 0.8158 - val_accuracy: 0.7139\n",
      "Epoch 12/20\n",
      "184/184 [==============================] - 102s 552ms/step - loss: 0.6765 - accuracy: 0.7766 - val_loss: 0.8630 - val_accuracy: 0.7221\n",
      "Epoch 13/20\n",
      "184/184 [==============================] - 102s 553ms/step - loss: 0.6529 - accuracy: 0.7851 - val_loss: 0.8617 - val_accuracy: 0.7084\n",
      "Epoch 14/20\n",
      "184/184 [==============================] - 103s 557ms/step - loss: 0.6157 - accuracy: 0.8042 - val_loss: 0.7950 - val_accuracy: 0.7411\n",
      "Epoch 15/20\n",
      "184/184 [==============================] - 106s 576ms/step - loss: 0.5981 - accuracy: 0.8164 - val_loss: 0.7663 - val_accuracy: 0.7439\n",
      "Epoch 16/20\n",
      "184/184 [==============================] - 106s 575ms/step - loss: 0.5704 - accuracy: 0.8222 - val_loss: 0.8557 - val_accuracy: 0.7357\n",
      "Epoch 17/20\n",
      "184/184 [==============================] - 108s 585ms/step - loss: 0.5659 - accuracy: 0.8222 - val_loss: 0.8024 - val_accuracy: 0.7275\n",
      "Epoch 18/20\n",
      "184/184 [==============================] - 109s 592ms/step - loss: 0.5485 - accuracy: 0.8243 - val_loss: 0.8050 - val_accuracy: 0.7548\n",
      "Epoch 19/20\n",
      "184/184 [==============================] - 114s 619ms/step - loss: 0.5329 - accuracy: 0.8406 - val_loss: 0.7797 - val_accuracy: 0.7357\n",
      "Epoch 20/20\n",
      "184/184 [==============================] - 113s 613ms/step - loss: 0.5221 - accuracy: 0.8324 - val_loss: 0.8152 - val_accuracy: 0.7548\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(train_dataset,\n",
    "                    epochs=20, \n",
    "                    validation_data=val_dataset,\n",
    "                    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/23 [==============================] - 2s 67ms/step - loss: 0.7168 - accuracy: 0.7575\n",
      "Test Loss: 0.7167800068855286\n",
      "Test Accuracy: 0.7574931979179382\n"
     ]
    }
   ],
   "source": [
    "test_loss, test_accuracy = model.evaluate(test_dataset)\n",
    "\n",
    "# Print the test loss and accuracy\n",
    "print('Test Loss:', test_loss)\n",
    "print('Test Accuracy:', test_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "model1 = Sequential([\n",
    "  data_augmentation,\n",
    "  layers.Conv2D(16, (3, 3), activation='relu'),\n",
    "  layers.MaxPooling2D((2, 2)),\n",
    "  layers.Conv2D(32, (3, 3), activation='relu'),\n",
    "  layers.MaxPooling2D((2, 2)),\n",
    "  layers.Conv2D(64, (3, 3), activation='relu'),\n",
    "  layers.MaxPooling2D((2, 2)),\n",
    "  layers.Conv2D(64, (3, 3), activation='relu'),\n",
    "  layers.MaxPooling2D((5, 5)),\n",
    "  layers.Flatten(),\n",
    "  layers.Dense(64, activation='relu', activity_regularizer=regularizers.L1(0.01)),\n",
    "  layers.Dropout(0.1),\n",
    "  layers.Dense(64, activation='tanh'),\n",
    "  layers.Dense(5, name=\"outputs\", activation=\"softmax\")\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "model1.compile(optimizer=\"adam\", loss='sparse_categorical_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "161/161 [==============================] - 69s 422ms/step - loss: 1.4325 - accuracy: 0.3671 - val_loss: 1.2770 - val_accuracy: 0.4745\n",
      "Epoch 2/10\n",
      "161/161 [==============================] - 74s 458ms/step - loss: 1.1563 - accuracy: 0.5434 - val_loss: 1.1043 - val_accuracy: 0.5600\n",
      "Epoch 3/10\n",
      "161/161 [==============================] - 74s 460ms/step - loss: 1.0447 - accuracy: 0.6061 - val_loss: 0.9479 - val_accuracy: 0.6455\n",
      "Epoch 4/10\n",
      "161/161 [==============================] - 76s 469ms/step - loss: 0.9511 - accuracy: 0.6469 - val_loss: 0.9370 - val_accuracy: 0.6600\n",
      "Epoch 5/10\n",
      "161/161 [==============================] - 74s 460ms/step - loss: 0.9069 - accuracy: 0.6734 - val_loss: 0.9899 - val_accuracy: 0.6382\n",
      "Epoch 6/10\n",
      "161/161 [==============================] - 73s 454ms/step - loss: 0.8714 - accuracy: 0.6878 - val_loss: 0.8943 - val_accuracy: 0.6800\n",
      "Epoch 7/10\n",
      "161/161 [==============================] - 75s 462ms/step - loss: 0.8388 - accuracy: 0.6851 - val_loss: 0.8284 - val_accuracy: 0.7127\n",
      "Epoch 8/10\n",
      "161/161 [==============================] - 79s 490ms/step - loss: 0.8196 - accuracy: 0.7018 - val_loss: 0.8207 - val_accuracy: 0.7109\n",
      "Epoch 9/10\n",
      "161/161 [==============================] - 76s 472ms/step - loss: 0.7941 - accuracy: 0.7018 - val_loss: 0.9051 - val_accuracy: 0.6455\n",
      "Epoch 10/10\n",
      "161/161 [==============================] - 80s 493ms/step - loss: 0.7645 - accuracy: 0.7197 - val_loss: 0.8480 - val_accuracy: 0.6891\n"
     ]
    }
   ],
   "source": [
    "history = model1.fit(train_dataset,\n",
    "                    epochs=10, \n",
    "                    validation_data=val_dataset,\n",
    "                    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "35/35 [==============================] - 2s 53ms/step - loss: 0.8205 - accuracy: 0.7042\n",
      "Test Loss: 0.8205288648605347\n",
      "Test Accuracy: 0.7041742205619812\n"
     ]
    }
   ],
   "source": [
    "test_loss, test_accuracy = model1.evaluate(test_dataset)\n",
    "\n",
    "# Print the test loss and accuracy\n",
    "print('Test Loss:', test_loss)\n",
    "print('Test Accuracy:', test_accuracy)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
